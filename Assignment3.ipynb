{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9a42d3e",
   "metadata": {
    "id": "a9a42d3e"
   },
   "source": [
    "## AISE4010- Assignment3 - Time Series Classification using TCN and Transformer + Hyperparameter Tuning\n",
    "\n",
    "## Grade: 100 points\n",
    "\n",
    "### Instructions\n",
    "\n",
    "#### Follow These Steps before submitting your assignment\n",
    "\n",
    "1. Complete the notebook.\n",
    "\n",
    "2. Make sure all plots have axis labels.\n",
    "\n",
    "3. Once the notebook is complete, `Restart` your kernel by clicking 'Kernel' > 'Restart & Run All'.\n",
    "\n",
    "4. Fix any errors until your notebook runs without any problems.\n",
    "\n",
    "5. Submit one completed notebook for the group to OWL by the deadline.\n",
    "\n",
    "6. Make sure to reference all external code and documentation used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5052396f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras_tuner'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 10\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (Conv1D, GlobalAveragePooling1D, Dense, \n\u001b[0;32m      7\u001b[0m                                       Dropout, Flatten, MultiHeadAttention, \n\u001b[0;32m      8\u001b[0m                                       LayerNormalization, Activation)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Adam\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras_tuner\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GridSearch\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m     12\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras_tuner'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Sequential\n",
    "from tensorflow.keras.layers import (Conv1D, GlobalAveragePooling1D, Dense, \n",
    "                                      Dropout, Flatten, MultiHeadAttention, \n",
    "                                      LayerNormalization, Activation)\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras_tuner import GridSearch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f0f3d3",
   "metadata": {
    "id": "b5f0f3d3"
   },
   "source": [
    "### Dataset\n",
    "\n",
    "The dataset is a sample of 46 satellite images, collected in 2006, located in southwestern France near Toulouse. It\n",
    "is a 24 km × 24 km area and the dataset uses 3 output classes (2 available) for arable soil classification based on the following paper: https://arxiv.org/pdf/1811.10166.\n",
    "\n",
    "You will be using helper functions below to prepare it for deep learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03632d8e-12d3-4200-a8fa-54b9d3e6ac98",
   "metadata": {
    "id": "03632d8e-12d3-4200-a8fa-54b9d3e6ac98"
   },
   "outputs": [],
   "source": [
    "# Call this helper method by passing in the names of the provided training and test sets' files.\n",
    "def read_SITS_data(name_file):\n",
    "    data = pd.read_table(name_file, sep=',', header=None)\n",
    "\n",
    "    y_data = data.iloc[:,0]\n",
    "    y = np.asarray(y_data.values, dtype='uint8')\n",
    "    y[y>1] = 0\n",
    "\n",
    "    polygonID_data = data.iloc[:,1]\n",
    "    polygon_ids = polygonID_data.values\n",
    "    polygon_ids = np.asarray(polygon_ids, dtype='uint16')\n",
    "\n",
    "    X_data = data.iloc[:,2:]\n",
    "    X = X_data.values\n",
    "    X = np.asarray(X, dtype='float32')\n",
    "\n",
    "    return  X, polygon_ids, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4517e085-607d-4113-a9e0-de4864603bb9",
   "metadata": {
    "id": "4517e085-607d-4113-a9e0-de4864603bb9"
   },
   "outputs": [],
   "source": [
    "def custom_feature_scaling(train, test):\n",
    "    min_per = np.percentile(train, 2, axis=(0,1))\n",
    "    max_per = np.percentile(train, 100-2, axis=(0,1))\n",
    "\n",
    "    new_train = (train-min_per)/(max_per-min_per)\n",
    "    new_test = (test-min_per)/(max_per-min_per)\n",
    "\n",
    "    return new_train, new_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d33d79a",
   "metadata": {
    "id": "3d33d79a"
   },
   "source": [
    "### Question 1 - Data Preprocessing (15%)\n",
    "- Q1.1 Call \"read_SITS_data()\" for the training set and store the results as X_train, polygon_ids_train, and y_train.\n",
    "- Q1.2 Call \"read_SITS_data()\" above for the test set and store the results as X_test, polygon_ids_test, and y_test.\n",
    "- Q1.3 Reshape the training and test sets.\n",
    "  - Each set must be reshaped into a 3-D array. The first dimension will be the number of rows of the original set. The second dimension will be int(x / 3), where x is the number of columns of the original set and int() is a casting function. The third dimension will be 3 (number of channels).\n",
    "- Q1.4 Call \"custom_feature_scaling()\" with the training and test sets. Save the results as the final sets for use.\n",
    "- Q1.5 How many entries are in the training set? How many time steps are in each entry? How many features are there for each time step? How many labels for each entry?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cfb1ce-0c21-4243-b221-f8d06145bf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1.1 - Load training data\n",
    "X_train, polygon_ids_train, y_train = read_SITS_data('train_dataset.csv')\n",
    "\n",
    "# Q1.2 - Load test data\n",
    "X_test, polygon_ids_test, y_test = read_SITS_data('test_dataset.csv')\n",
    "\n",
    "print(f\"X_train shape before reshape: {X_train.shape}\")\n",
    "print(f\"X_test shape before reshape: {X_test.shape}\")\n",
    "\n",
    "# Q1.3 - Reshape data into 3D arrays\n",
    "# First dimension: number of rows\n",
    "# Second dimension: int(columns / 3)\n",
    "# Third dimension: 3 (number of channels)\n",
    "n_train = X_train.shape[0]\n",
    "n_test = X_test.shape[0]\n",
    "n_features = X_train.shape[1]\n",
    "time_steps = int(n_features / 3)\n",
    "\n",
    "X_train = X_train.reshape(n_train, time_steps, 3)\n",
    "X_test = X_test.reshape(n_test, time_steps, 3)\n",
    "\n",
    "print(f\"X_train shape after reshape: {X_train.shape}\")\n",
    "print(f\"X_test shape after reshape: {X_test.shape}\")\n",
    "\n",
    "# Q1.4 - Apply custom feature scaling\n",
    "X_train, X_test = custom_feature_scaling(X_train, X_test)\n",
    "\n",
    "print(f\"\\nFinal X_train shape: {X_train.shape}\")\n",
    "print(f\"Final X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b85f41",
   "metadata": {
    "id": "f3b85f41"
   },
   "source": [
    "*Write your Answer to Q1.5 here:*\n",
    "\n",
    "**Q1.5 Answer:**\n",
    "- **Number of entries in the training set:** 46 (the number of samples/satellite images)\n",
    "- **Number of time steps in each entry:** 24 (derived from 72 columns / 3 = 24 time steps)\n",
    "- **Number of features for each time step:** 3 (the three channels representing different spectral bands)\n",
    "- **Number of labels for each entry:** 1 (binary classification label per entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d501e5c3",
   "metadata": {
    "id": "d501e5c3"
   },
   "source": [
    "### Question2 - Temporal Convolutional Network\n",
    "- Q2.1 Create a Sequential model for classification. The model should have a TCN layer of size 64, a fully connected layer of size 256, a dropout of 0.3, and a fully connected output layer with Softmax activation (Hint: the logits axis should be on 0). Train the model using the provided dataset for 20 epochs. Use the batch_size of 32, and ADAM optimizer. Print the model summary.\n",
    "- Q2.2 Train the model with the same parameters, print the model summary and evaluate the model's accuracy on the test set. Print the accuracy.\n",
    "- Q2.3 Why do we use the Softmax activation on the output layer? In what scenarios does this contrast to using ReLU instead?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984215d3-9331-4d76-a970-ef3208bd17de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2.1 and Q2.2 - Create and train TCN model\n",
    "from tensorflow.keras.layers import TCN\n",
    "\n",
    "# Create Sequential model with TCN\n",
    "model_tcn = Sequential([\n",
    "    TCN(64, kernel_size=5, dilations=[1, 2, 4, 8], input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "    Dense(256),\n",
    "    Dropout(0.3),\n",
    "    Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "# Print model summary\n",
    "print(\"=\" * 60)\n",
    "print(\"TCN Model Summary:\")\n",
    "print(\"=\" * 60)\n",
    "model_tcn.summary()\n",
    "\n",
    "# Compile and train the model\n",
    "model_tcn.compile(\n",
    "    optimizer=Adam(),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"\\nTraining TCN Model...\")\n",
    "history_tcn = model_tcn.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=32,\n",
    "    epochs=20,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate on test set\n",
    "print(\"\\nEvaluating TCN Model on Test Set...\")\n",
    "test_loss, test_accuracy = model_tcn.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"TCN Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55549340",
   "metadata": {
    "id": "55549340"
   },
   "source": [
    "*Write your Answer to Q2.3 Here:*\n",
    "\n",
    "**Q2.3 Answer:**\n",
    "\n",
    "**Why Softmax on Output Layer:**\n",
    "Softmax activation is used on the output layer for multi-class classification problems because it:\n",
    "1. Converts raw logits into probability distributions that sum to 1\n",
    "2. Ensures each output neuron represents the probability of a specific class\n",
    "3. Makes it suitable for use with categorical/sparse_categorical_crossentropy loss functions\n",
    "4. Handles multiple mutually exclusive classes where only one label should be true\n",
    "\n",
    "**Contrast with ReLU:**\n",
    "- **ReLU** (Rectified Linear Unit) outputs unbounded positive values (max(0, x)), which don't naturally form probabilities and can sum to any positive value\n",
    "- **ReLU** is inappropriate for classification output layers as it doesn't constrain outputs to probability ranges [0,1]\n",
    "- **ReLU** is ideal for hidden layers to introduce non-linearity and allow the network to learn complex patterns\n",
    "- Using ReLU on output would require additional normalization to interpret results as probabilities\n",
    "\n",
    "In summary, Softmax ensures valid probability distributions for classification, while ReLU is better suited for hidden layer activations in deep networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf4f585",
   "metadata": {
    "id": "4cf4f585"
   },
   "source": [
    "### Question 3 - Transformer Model\n",
    "- Q3.1 Create a transformer encoder block. It should use MultiHeadAttention for residual connection. The projection layers can be two Conv1D layers, based on number of feed forward dimensions and with kernel sizes of 1.\n",
    "- Q3.2 Define the model. It should have 4 encoder blocks, each with 256 heads and feed forward dimensions of 4. Add a flatten layer, then a fully connected layer of size 2 and a fully connected output layer.\n",
    "- Q3.3 Print the model summary, train the model using 50 epochs and a batch size of 32. Evaluate the model accuracy on the test set and print it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e16ef6e-2305-4f0c-846f-90e1fce36760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3.1 - Create Transformer encoder block with MultiHeadAttention\n",
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    \"\"\"\n",
    "    Transformer encoder block with MultiHeadAttention and residual connection\n",
    "    \n",
    "    Args:\n",
    "        inputs: input tensor\n",
    "        head_size: size of each attention head\n",
    "        num_heads: number of attention heads\n",
    "        ff_dim: feed forward dimensions\n",
    "        dropout: dropout rate\n",
    "    \"\"\"\n",
    "    # Multi-head attention with residual connection\n",
    "    x = MultiHeadAttention(\n",
    "        num_heads=num_heads, \n",
    "        key_dim=head_size, \n",
    "        dropout=dropout\n",
    "    )(inputs, inputs)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = LayerNormalization(epsilon=1e-6)(x + inputs)\n",
    "    \n",
    "    # Feed forward network with residual connection\n",
    "    x_res = x\n",
    "    x = Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = LayerNormalization(epsilon=1e-6)(x + x_res)\n",
    "    \n",
    "    return x\n",
    "\n",
    "# Q3.2 - Define the complete Transformer model\n",
    "def create_transformer_model(input_shape, num_transformer_blocks=4, head_size=256, \n",
    "                             num_heads=4, ff_dim=4, dropout=0.1):\n",
    "    \"\"\"\n",
    "    Create a transformer model for time series classification\n",
    "    \"\"\"\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    x = inputs\n",
    "    \n",
    "    # Apply 4 transformer encoder blocks\n",
    "    for _ in range(num_transformer_blocks):\n",
    "        x = transformer_encoder(x, head_size, num_heads, ff_dim * input_shape[-1], dropout)\n",
    "    \n",
    "    # Flatten and fully connected layers\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(2)(x)\n",
    "    outputs = Dense(2, activation=\"softmax\")(x)\n",
    "    \n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "model_transformer = create_transformer_model(\n",
    "    input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "    num_transformer_blocks=4,\n",
    "    head_size=256,\n",
    "    num_heads=4,\n",
    "    ff_dim=4,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "# Q3.3 - Print model summary, train, and evaluate\n",
    "print(\"=\" * 60)\n",
    "print(\"Transformer Model Summary:\")\n",
    "print(\"=\" * 60)\n",
    "model_transformer.summary()\n",
    "\n",
    "# Compile the model\n",
    "model_transformer.compile(\n",
    "    optimizer=Adam(),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"\\nTraining Transformer Model...\")\n",
    "history_transformer = model_transformer.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=32,\n",
    "    epochs=50,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate on test set\n",
    "print(\"\\nEvaluating Transformer Model on Test Set...\")\n",
    "test_loss_trans, test_accuracy_trans = model_transformer.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Transformer Test Accuracy: {test_accuracy_trans:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3836425",
   "metadata": {
    "id": "c3836425"
   },
   "source": [
    "### Question 4 - Hyperparameter Tuning\n",
    "- Q4.1 Define a search space for the number of neurons in the fully connected layer that follows the flatten layer. The lower bound should be 2, the upper bound should be 16, and it should search every other value in between. Also have the tuner decide whether or not a dropout layer of 0.3 should be added after the aforementioned layer.\n",
    "- Q4.2 Using GridSearch, search for the best hyperparameters with respect to accuracy over 50 epochs.\n",
    "- Q4.3 Using the best hyperparameters, rebuild the model and print the model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb0bcdc-45d7-44ba-bbef-50be40c75641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4.1 - Define the model builder function with hyperparameter tuning\n",
    "def build_model(hp):\n",
    "    \"\"\"\n",
    "    Build a transformer model with tunable hyperparameters\n",
    "    \"\"\"\n",
    "    # Define search space for number of neurons in fully connected layer\n",
    "    # Lower bound: 2, Upper bound: 16, search every other value (step=2)\n",
    "    fc_units = hp.Int('fc_units', min_value=2, max_value=16, step=2)\n",
    "    \n",
    "    # Define whether to add dropout layer\n",
    "    use_dropout = hp.Boolean('use_dropout')\n",
    "    \n",
    "    # Build model\n",
    "    inputs = keras.Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
    "    x = inputs\n",
    "    \n",
    "    # Transformer encoder blocks\n",
    "    for _ in range(2):\n",
    "        x = transformer_encoder(x, 128, 2, 4 * X_train.shape[2], dropout=0.1)\n",
    "    \n",
    "    # Flatten\n",
    "    x = Flatten()(x)\n",
    "    \n",
    "    # Fully connected layer with tunable units\n",
    "    x = Dense(fc_units)(x)\n",
    "    \n",
    "    # Optional dropout\n",
    "    if use_dropout:\n",
    "        x = Dropout(0.3)(x)\n",
    "    \n",
    "    # Output layer\n",
    "    outputs = Dense(2, activation='softmax')(x)\n",
    "    \n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    # Compile\n",
    "    model.compile(\n",
    "        optimizer=Adam(),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Q4.2 - Use GridSearch to find best hyperparameters\n",
    "print(\"=\" * 60)\n",
    "print(\"Starting GridSearch for Hyperparameter Tuning...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "tuner = GridSearch(\n",
    "    build_model,\n",
    "    objective='accuracy',\n",
    "    max_trials=24,  # 2 options for dropout × 12 values for fc_units (2,4,6,8,10,12,14,16)\n",
    "    directory='grid_search_results',\n",
    "    project_name='transformer_tuning'\n",
    ")\n",
    "\n",
    "print(\"\\nRunning GridSearch...\")\n",
    "tuner.search(\n",
    "    X_train, y_train,\n",
    "    batch_size=32,\n",
    "    epochs=50,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(f\"\\nBest Hyperparameters Found:\")\n",
    "print(f\"  FC Units: {best_hps.get('fc_units')}\")\n",
    "print(f\"  Use Dropout: {best_hps.get('use_dropout')}\")\n",
    "\n",
    "# Q4.3 - Rebuild model with best hyperparameters and evaluate\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Building Final Model with Best Hyperparameters...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "best_model = tuner.hypermodel.build(best_hps)\n",
    "print(\"\\nBest Model Summary:\")\n",
    "best_model.summary()\n",
    "\n",
    "print(\"\\nTraining Best Model...\")\n",
    "best_model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=32,\n",
    "    epochs=50,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Evaluate on test set\n",
    "print(\"\\nEvaluating Best Model on Test Set...\")\n",
    "best_test_loss, best_test_accuracy = best_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Best Model Test Accuracy: {best_test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bb2822",
   "metadata": {
    "id": "18bb2822"
   },
   "source": [
    "### Question 6 - Discussion (5%)\n",
    "- Q6.1 Indicate other hyperparameters relevant to transformers that can be tuned.\n",
    "- Q6.2 What are the advantages and disadvantages of using GridSearch for finding optimal hyperparameters?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b39d8ed",
   "metadata": {
    "id": "4b39d8ed"
   },
   "source": [
    "*Write your Answer to Q6.1 and Q6.2 Here:*\n",
    "\n",
    "**Q6.1 - Other Hyperparameters Relevant to Transformers:**\n",
    "\n",
    "Key transformer hyperparameters that can be tuned include:\n",
    "\n",
    "1. **Number of Attention Heads:** Controls the dimensionality of attention mechanisms. More heads capture different aspects of relationships in the data.\n",
    "\n",
    "2. **Embedding Dimension (d_model):** The size of the feature vectors used throughout the transformer. Larger dimensions can capture more information but require more computation.\n",
    "\n",
    "3. **Feed Forward Dimension (ff_dim):** The hidden layer size in the feed-forward networks within encoder blocks. Typically 2-4 times the embedding dimension.\n",
    "\n",
    "4. **Number of Encoder Layers:** Stacking more transformer blocks increases model capacity and depth but also increases computational cost.\n",
    "\n",
    "5. **Dropout Rate:** Controls regularization strength to prevent overfitting. Common values range from 0.1 to 0.5.\n",
    "\n",
    "6. **Learning Rate:** Controls the step size during optimization. Affects convergence speed and final model performance.\n",
    "\n",
    "7. **Batch Size:** Impacts training stability and memory usage.\n",
    "\n",
    "8. **Head Size:** The dimensionality of each individual attention head (d_model / num_heads).\n",
    "\n",
    "9. **Activation Functions:** Choice of activation functions in feed-forward networks (ReLU, GELU, etc.).\n",
    "\n",
    "10. **Layer Normalization Position:** Pre-normalization vs post-normalization configuration.\n",
    "\n",
    "---\n",
    "\n",
    "**Q6.2 - Advantages and Disadvantages of GridSearch:**\n",
    "\n",
    "**Advantages:**\n",
    "- **Exhaustive Search:** Guarantees finding the best combination within the defined search space\n",
    "- **Simplicity:** Easy to understand and implement\n",
    "- **Reproducibility:** Deterministic results across runs\n",
    "- **Interpretability:** Can easily see how each hyperparameter affects performance\n",
    "- **Parallelizable:** Can run multiple trials in parallel on distributed systems\n",
    "- **No Assumptions:** Doesn't assume any functional relationship between hyperparameters and performance\n",
    "\n",
    "**Disadvantages:**\n",
    "- **Computational Expense:** Exponentially increases with number of hyperparameters (curse of dimensionality)\n",
    "  - Example: 3 hyperparameters with 10 values each = 1,000 trials\n",
    "- **Inefficient Search:** Tests uninformative combinations, wasting computational resources\n",
    "- **Scalability Issues:** Impractical for large search spaces or many hyperparameters\n",
    "- **Time-Consuming:** Can take hours or days for large grids with complex models\n",
    "- **Limited Scope:** Only searches discrete predefined values, missing optimal continuous values between them\n",
    "- **No Learning:** Doesn't use information from previous trials to inform the next search\n",
    "- **Coarse Granularity:** Resolution depends on number of predefined values\n",
    "\n",
    "**Better Alternatives:**\n",
    "- **Random Search:** More efficient for large search spaces\n",
    "- **Bayesian Optimization:** Uses probabilistic models to guide search\n",
    "- **Hyperband/Successive Halving:** Progressively eliminates poor performing configurations"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
