{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9a42d3e",
   "metadata": {
    "id": "a9a42d3e"
   },
   "source": [
    "## AISE4010- Assignment2 - Time Series Forecasting using CNN, RNN, LSTM and GRU\n",
    "\n",
    "## Grade: 100 points\n",
    "\n",
    "### Instructions\n",
    "\n",
    "#### Follow These Steps before submitting your assignment\n",
    "\n",
    "1. Complete the notebook.\n",
    "\n",
    "2. Make sure all plots have axis labels.\n",
    "\n",
    "3. Once the notebook is complete, `Restart` your kernel by clicking 'Kernel' > 'Restart & Run All'.\n",
    "\n",
    "4. Fix any errors until your notebook runs without any problems.\n",
    "\n",
    "5. Submit one completed notebook for the group to OWL by the deadline.\n",
    "\n",
    "6. Make sure to reference all external code and documentation used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f0f3d3",
   "metadata": {
    "id": "b5f0f3d3"
   },
   "source": [
    "### Dataset\n",
    "\n",
    "The `StockData.csv` represents a time series dataset designed to simulate the behavior of stock prices over a period of 1000 days, starting from January 1, 2023. This dataset includes key features commonly found in stock market data, such as opening prices, closing prices, daily highs, daily lows, and trading volume.\n",
    "\n",
    "Features:\n",
    "- Date: The timestamp representing the specific day for each record.\n",
    "- Open: The stock's price at the start of the trading day.\n",
    "- High: The highest price of the stock during the trading day. It is generally higher than the opening price, reflecting intraday market movements.\n",
    "- Low: The lowest price of the stock during the trading day. It is usually lower than the opening price, capturing downward movements in the market.\n",
    "- Close: The stockâ€™s price at the end of the trading day. This price reflects the market's closing valuation and fluctuates within a range close to the opening price.\n",
    "- Volume: The number of shares traded during the day."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d33d79a",
   "metadata": {
    "id": "3d33d79a"
   },
   "source": [
    "### Question 1 - Data Preprocessing (15%)\n",
    "- Q1.1 Load the provided dataset and plot the `Closing prices` time series. (5%)\n",
    "- Q1.2 (5%):\n",
    "\n",
    "  - a) Decompose the time series into its components and explain them.  \n",
    "  - b) What do the trend and seasonality components tell you about the overall behavior of the time series? How does understanding     these components help in selecting a model for time series forecasting?\n",
    "- Q1.3 Restructure the time series using a sliding window approach (with size 10) to prepare it for supervised learning. Split the `Closing prices` time series into 80% for training and 20% for testing. (5%)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f388ca4b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 844
    },
    "id": "f388ca4b",
    "outputId": "929260a7-e6be-4962-c28e-4c70b6561b77"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      4\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStockData.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\__init__.py:22\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m _hard_dependencies, _dependency, _missing_dependencies\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# numpy compat\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_numpy_dev \u001b[38;5;28;01mas\u001b[39;00m _is_numpy_dev  \u001b[38;5;66;03m# pyright: ignore # noqa:F401\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m hashtable \u001b[38;5;28;01mas\u001b[39;00m _hashtable, lib \u001b[38;5;28;01mas\u001b[39;00m _lib, tslib \u001b[38;5;28;01mas\u001b[39;00m _tslib\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\compat\\__init__.py:25\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_constants\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     18\u001b[0m     IS64,\n\u001b[0;32m     19\u001b[0m     PY39,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     22\u001b[0m     PYPY,\n\u001b[0;32m     23\u001b[0m )\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompressors\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     26\u001b[0m     is_numpy_dev,\n\u001b[0;32m     27\u001b[0m     np_version_under1p21,\n\u001b[0;32m     28\u001b[0m )\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyarrow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     30\u001b[0m     pa_version_under7p0,\n\u001b[0;32m     31\u001b[0m     pa_version_under8p0,\n\u001b[0;32m     32\u001b[0m     pa_version_under9p0,\n\u001b[0;32m     33\u001b[0m     pa_version_under11p0,\n\u001b[0;32m     34\u001b[0m )\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_function_name\u001b[39m(f: F, name: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mcls\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m F:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\compat\\numpy\\__init__.py:4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\" support numpy compatibility across versions \"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Version\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# numpy versioning\u001b[39;00m\n\u001b[0;32m      7\u001b[0m _np_version \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39m__version__\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\util\\__init__.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# pyright: reportUnusedImport = false\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_decorators\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     Appender,\n\u001b[0;32m      4\u001b[0m     Substitution,\n\u001b[0;32m      5\u001b[0m     cache_readonly,\n\u001b[0;32m      6\u001b[0m )\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhashing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     hash_array,\n\u001b[0;32m     10\u001b[0m     hash_pandas_object,\n\u001b[0;32m     11\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:14\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      7\u001b[0m     Any,\n\u001b[0;32m      8\u001b[0m     Callable,\n\u001b[0;32m      9\u001b[0m     Mapping,\n\u001b[0;32m     10\u001b[0m     cast,\n\u001b[0;32m     11\u001b[0m )\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mproperties\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cache_readonly\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_typing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     16\u001b[0m     F,\n\u001b[0;32m     17\u001b[0m     T,\n\u001b[0;32m     18\u001b[0m )\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_exceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find_stack_level\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\__init__.py:13\u001b[0m\n\u001b[0;32m      1\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNaT\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNaTType\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterval\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     10\u001b[0m ]\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minterval\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Interval\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtslibs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     15\u001b[0m     NaT,\n\u001b[0;32m     16\u001b[0m     NaTType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     21\u001b[0m     iNaT,\n\u001b[0;32m     22\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\interval.pyx:1\u001b[0m, in \u001b[0;36minit pandas._libs.interval\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv('StockData.csv')\n",
    "\n",
    "print(\"Q1.1 â€” Missing Values Summary\\n\")\n",
    "\n",
    "dfNull = df.isnull().sum()\n",
    "\n",
    "print(dfNull)\n",
    "\n",
    "if dfNull.sum() == 0:\n",
    "  print(\"\\nNo missing values detected â€” proceeding without any cleaning.\\n\")\n",
    "else :\n",
    "  print(\"\\nMissing values detected\")\n",
    "\n",
    "df['Date'] = pd.to_datetime(df['Date'], infer_datetime_format=True)\n",
    "df = df.sort_values('Date').reset_index(drop=True)\n",
    "\n",
    "ts = df.set_index('Date')[['Close']]\n",
    "\n",
    "print(\"Q1.1 â€” Dataset Summary\\n\")\n",
    "print(f\"Rows available: {len(ts)}\")\n",
    "print(f\"Date range: {ts.index[0].date()}  â†’  {ts.index[-1].date()}\\n\")\n",
    "print(ts.head(3))\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(ts.index, ts['Close'])\n",
    "plt.title(\"Closing Prices Over Time\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Close\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "m3s6OYxSEqON",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "m3s6OYxSEqON",
    "outputId": "3ed88697-efee-411d-dca4-0b0c42fadc9c"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "period = 7  # ~ one trading week\n",
    "\n",
    "# 1) Trend: centered 7-day moving average\n",
    "\n",
    "#   Trend = (Close_{t-3} + ... Close_{t+3}) / 7\n",
    "#         - The first/last 3 rows have no centered window so Nan (expected)\n",
    "\n",
    "trend = ts['Close'].rolling(window=period, center=True).mean()\n",
    "\n",
    "# 2) Seasonality: average detrended value at each cycle position (0..6)\n",
    "#    2.1 - Detrend: how much each point deviates from its local level (value provided around that day from the trend)\n",
    "#        - Detrend = Close - Trend\n",
    "\n",
    "detrended = ts['Close'] - trend\n",
    "\n",
    "#    2.2 = Assign each row a cycle position p(t) = t mod 7\n",
    "\n",
    "pos = np.arange(len(ts)) % period\n",
    "\n",
    "#    2.3 - For each position k in {0...6}, average the dtrended values across the whole series\n",
    "#        - S_index[k] = mean{Detrended | pos(t) = k}\n",
    "\n",
    "seasonal_index = pd.Series(detrended.values, index=pos).groupby(level=0).mean()\n",
    "\n",
    "#    2.4 - Enforce additive zero-mean seasonality over one full cycle:\n",
    "#        - S_index[k] = S_index[k] - [sum_j S_index[j]]/7\n",
    "\n",
    "seasonal_index -= seasonal_index.mean()\n",
    "\n",
    "#    2.5 - Map the 5 indices back to every row via its position:\n",
    "#        - Seasonal = S_index[pos(t)]\n",
    "\n",
    "seasonal = seasonal_index.reindex(pos).to_numpy()\n",
    "\n",
    "# 3) Residual\n",
    "\n",
    "#   Residual = Close - Trend - Seasonal\n",
    "#            = Residual will be NaN where Trend is NaN\n",
    "\n",
    "residual = ts['Close'].to_numpy() - trend.to_numpy() - seasonal\n",
    "\n",
    "# 4) Pack results for inspection / plotting\n",
    "decomp = pd.DataFrame(\n",
    "    {'Observed': ts['Close'].to_numpy(),   #Original series Close\n",
    "     'Trend': trend.to_numpy(),            #Centered 7-day moving average\n",
    "     'Seasonal': seasonal,                 #Repeating 7-step pattern\n",
    "     'Residual': residual},\n",
    "    index=ts.index\n",
    ")\n",
    "\n",
    "# Quick preview\n",
    "print(\"Q1.2 â€” Decomposition (head):\")\n",
    "print(decomp.head(14))\n",
    "\n",
    "# Analyze components for discussion\n",
    "\n",
    "#1. Fit a line on the valid Trend to see if its rising or falling\n",
    "trend_valid = decomp['Trend'].dropna()\n",
    "\n",
    "if len(trend_valid) > 8:\n",
    "\n",
    "    x = np.arange(len(trend_valid))\n",
    "    slope = np.polyfit(x, trend_valid.values, 1)[0]\n",
    "    trend_dir = \"upward\" if slope > 0 else (\"downward\" if slope < 0 else \"flat\")\n",
    "\n",
    "else:\n",
    "\n",
    "    trend_dir = \"n/a\"\n",
    "\n",
    "#2. Seasonal Amplitude: Peak-to-peak size of the repeating 7-step pattern\n",
    "seasonal_amp = float(np.nanmax(decomp['Seasonal']) - np.nanmin(decomp['Seasonal']))\n",
    "\n",
    "#3. Residual std: how noisy the unexplained remainder is\n",
    "resid_std = float(np.nanstd(decomp['Residual']))\n",
    "\n",
    "print(f\"\\nTrend direction (slope sign): {trend_dir}\")\n",
    "print(f\"Seasonal amplitude (peak-to-peak, 7-step cycle): ~{seasonal_amp:.3f}\")\n",
    "print(f\"Residual std: ~{resid_std:.3f}\")\n",
    "\n",
    "# 5) Plots (simple, consistent)\n",
    "plt.figure(figsize=(12, 3))\n",
    "plt.plot(decomp.index, decomp['Observed'])\n",
    "plt.title(\"Observed (Close)\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 3))\n",
    "plt.plot(decomp.index, decomp['Trend'])\n",
    "plt.title(\"Trend (Centered 7-day Moving Average)\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 3))\n",
    "plt.plot(decomp.index, decomp['Seasonal'])\n",
    "plt.title(\"Seasonal (7-position cycle)\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# NEW: Zoomed seasonal plot (first 28 days â‰ˆ 4 full cycles) for visual clarity\n",
    "sample_length = 28\n",
    "plt.figure(figsize=(12, 3))\n",
    "plt.plot(decomp.index[:sample_length], decomp['Seasonal'][:sample_length])\n",
    "plt.title(f\"Seasonal (First {sample_length} Days â€” Zoomed, 7-position cycle)\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 3))\n",
    "plt.plot(decomp.index, decomp['Residual'])\n",
    "plt.title(\"Residual = Observed âˆ’ Trend âˆ’ Seasonal\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Qm3UKKarBWCg",
   "metadata": {
    "id": "Qm3UKKarBWCg"
   },
   "source": [
    "#**Q1.2(a)** - Decomposition Components\n",
    "\n",
    "**Observed:** This is the raw closing price directly from the dataset - it includes the combined effect of long-term growth, short-term repetitive cycles, and noise.\n",
    "\n",
    "**Trend:** A smoothed version of the price that captures the long-term direction of movement, ignoring short-term fluctuations. In this dataset the trend shows a gradual upward increase over time, with a significant jump around early 2025, indicating strong long term market growth rather than random fluctuation.\n",
    "\n",
    "**Seasonal:** A repeating short-term 7-step pattern that cycles consistently over time. This reflects small but predicatble oscillations around the trend, likely due to systematic short-term behavior. The amplitude shows that seasonality influences short-term variation but does not dominate overall direction.\n",
    "\n",
    "**Residual:** Whatever remains after the trend and seasonality are removed - it represents noise caused by market shocks, news events, or random trading activity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-guuHSCJEi_p",
   "metadata": {
    "id": "-guuHSCJEi_p"
   },
   "source": [
    "#**Q1.2(b)** - Interpretation of Trend & Seasonality\n",
    "\n",
    "**Trend:** The trend shows a strong upward long-term movement, indicating the asset is generally growin in value over timie, with a major increase starting around 2025.\n",
    "\n",
    "**Seasonality:** The seasonal component exhibits a stable, repeating 7-day cycle, suggesting consistent short-term fluctuations that repeat regularly.\n",
    "\n",
    "****\n",
    "\n",
    "**Why This Matters For Forecasting**\n",
    "\n",
    "Since there is a clear upward trend, we should use a forecasting model that supports non-stationarity. Because seasonality is present and stable, a seasonality-aware model will perfrom better than a model that assumes the data has no repeating pattern. SARIMA or LSTM could be good models to use on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "R6KGqBGJkSGW",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R6KGqBGJkSGW",
    "outputId": "5f18cb7f-f856-478d-8a98-a19879827a65"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# 1) Extract Close series\n",
    "prices = ts['Close'].values.astype(float)\n",
    "\n",
    "# 2) Build sliding windows (window = 10)\n",
    "WINDOW = 10\n",
    "X, y = [], []\n",
    "for i in range(len(prices) - WINDOW):\n",
    "    X.append(prices[i : i + WINDOW])   # Input: Contains 10 consecutive past closing prices (day t to t+9)\n",
    "    y.append(prices[i + WINDOW])       # Output: The close immediately after those 10 days (day t+10)\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "print(\"Q1.3 â€” Raw supervised arrays\")\n",
    "print(f\"X shape (samples, window): {X.shape}\")\n",
    "print(f\"y shape (samples,):       {y.shape}\")\n",
    "\n",
    "# 3) 80/20 split INDEX\n",
    "#    - We are not shuffling the data because the time must remain sequential\n",
    "\n",
    "split_idx = int(len(X) * 0.8)\n",
    "\n",
    "# 4) Normalize (GLOBAL scaling: fit on TRAIN ONLY, apply to all)\n",
    "#    - MinMax to [0,1] (good for NN training stability)\n",
    "#    - Fit x_scaler on all values from TRAIN windows; fit y_scaler on TRAIN targets\n",
    "\n",
    "x_scaler = MinMaxScaler()\n",
    "y_scaler = MinMaxScaler()\n",
    "\n",
    "x_scaler.fit(X[:split_idx].reshape(-1, 1))\n",
    "y_scaler.fit(y[:split_idx].reshape(-1, 1))\n",
    "\n",
    "# Transform 2D window arrays with a single scaler\n",
    "def transform_windows(arr2d, scaler):\n",
    "    flat = arr2d.reshape(-1, 1)\n",
    "    flat_scaled = scaler.transform(flat)\n",
    "    return flat_scaled.reshape(arr2d.shape)\n",
    "\n",
    "X_scaled = transform_windows(X, x_scaler)\n",
    "y_scaled = y_scaler.transform(y.reshape(-1, 1))\n",
    "\n",
    "# 5) Final chronological split (no shuffling)\n",
    "X_train = X_scaled[:split_idx].copy()\n",
    "X_test  = X_scaled[split_idx:].copy()\n",
    "y_train = y_scaled[:split_idx].copy()\n",
    "y_test  = y_scaled[split_idx:].copy()\n",
    "\n",
    "print(\"\\nQ1.3 â€” Train/Test split (80/20 chronological)\")\n",
    "print(f\"Training set shape: X_train {X_train.shape}, y_train {y_train.shape}\")\n",
    "print(f\"Test set shape:     X_test  {X_test.shape},  y_test  {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d501e5c3",
   "metadata": {
    "id": "d501e5c3"
   },
   "source": [
    "### Question2 - 1D Convolutional Neural Network (CNN) (20%)\n",
    "- Q2.1 Create a 1D CNN model for time series forecasting. The model should have two convolutional layers of size 64 and 32 and kernel size of 3, fully connected layer of size 50, droup out of 0.3, and ReLU activation. Train the model using the provided dataset for 50 epochs. Use the batch_size of 32, and ADAM optimizer. Plot the training loss and validation loss. (12%)\n",
    "- Q2.2 Evaluate the model's performance using RMSE and MAE on the test set. Plot the predicted values alongside the actual values to visualize how well the model is forecasting the time series. (6%)\n",
    "- Q2.3 How does the 1D-CNN perform in terms of capturing short-term dependencies? Based on the RMSE and MAE values, do you think this model can effectively capture long-term trends? Why or why not? (2%)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebcda85",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "5ebcda85",
    "outputId": "1e902b07-af96-4b67-f2a3-3920533885d9"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "\n",
    "#1) Prepare shapes for Conv1D\n",
    "#   - Conv1D expects input: (samples, timesteps, features) so we must change our input to this format\n",
    "#         - (1, 10, 1): Each input sample is a 10-day sequence, with 1 value per day (the closing price)\n",
    "#   - We reshape our output to 1D - Keras prefers 1D targets for regression.\n",
    "\n",
    "WINDOW = X_train.shape[1]\n",
    "X_train_cnn = X_train.reshape(-1, WINDOW, 1)\n",
    "X_test_cnn  = X_test.reshape(-1, WINDOW, 1)\n",
    "\n",
    "y_train_vec = y_train.ravel()\n",
    "y_test_vec  = y_test.ravel()\n",
    "\n",
    "print(\"Q2.1 â€” Shapes ready for CNN\")\n",
    "print(f\"X_train_cnn: {X_train_cnn.shape}  (samples, timesteps={WINDOW}, features=1)\")\n",
    "print(f\"X_test_cnn: {X_test_cnn.shape}\")\n",
    "print(f\"y_train: {y_train_vec.shape}\")\n",
    "print(f\"y_test: {y_test_vec.shape}\")\n",
    "\n",
    "# 2) Chronological validation split from the training set\n",
    "#   - Used to monitor learning progress during training. We are using the last 10% of training for validation\n",
    "#   - Check's if the data is overfitting or underperforming. We keep the data in chronological order.\n",
    "\n",
    "val_start = int(len(X_train_cnn) * 0.9)\n",
    "X_tr, X_val = X_train_cnn[:val_start], X_train_cnn[val_start:]\n",
    "y_tr, y_val = y_train_vec[:val_start],  y_train_vec[val_start:]\n",
    "\n",
    "print(\"\\nQ2.1 â€” Train/Val split (from training set)\")\n",
    "print(f\"X_tr:  {X_tr.shape}, y_tr:  {y_tr.shape}\")\n",
    "print(f\"X_val: {X_val.shape}, y_val: {y_val.shape}\\n\")\n",
    "\n",
    "# 3) Define the 1D CNN per spec:\n",
    "#    - Conv1D: 64 filters, kernel_size=3, ReLU\n",
    "#    - Conv1D: 32 filters, kernel_size=3, ReLU\n",
    "#    - Dense:  50 units, ReLU    -  Learns nonlinear relationships between extracted features\n",
    "#    - Dropout: 0.3              -  Prevents overfitting by randomly dropping 30% of neurons\n",
    "#    - Output: 1 (regression)\n",
    "#    - Optimizer: Adam           -  Automatically adapts the learning rate during training.\n",
    "#    - Loss: MSE\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Conv1D(filters=64, kernel_size=3, activation='relu', padding='same', input_shape=(WINDOW, 1)),\n",
    "    layers.Conv1D(filters=32, kernel_size=3, activation='relu', padding='same'),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(50, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer=optimizers.Adam(), loss='mse', metrics=['mae'])\n",
    "model.summary()\n",
    "\n",
    "# 4) Train for 50 epochs, batch_size=32\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "history = model.fit(\n",
    "    X_tr, y_tr,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 5) Plot training vs validation loss (MSE)\n",
    "train_loss = history.history['loss']\n",
    "val_loss   = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(train_loss, label='Train Loss (MSE)')\n",
    "plt.plot(val_loss,   label='Validation Loss (MSE)')\n",
    "plt.title(\"Q2.1 â€” Training vs. Validation Loss (1D-CNN)\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss (MSE)\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1299aa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 749
    },
    "id": "7c1299aa",
    "outputId": "fdd1b4e4-1f21-46d3-826e-768c35cb1780"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# 1) Predict on the test set (still in [0,1] scale)\n",
    "\n",
    "y_pred_scaled = model.predict(X_test_cnn, verbose=0).ravel()\n",
    "\n",
    "# 2) Inverse-transform to original price scale\n",
    "#    - To interpret RMSE/MAE properly, we reverse the normalized scaling we applied in 1.3\n",
    "\n",
    "y_pred = y_scaler.inverse_transform(y_pred_scaled.reshape(-1, 1)).ravel()\n",
    "y_true = y_scaler.inverse_transform(y_test_vec.reshape(-1, 1)).ravel()\n",
    "\n",
    "# 3) Metrics in original units\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "mae  = mean_absolute_error(y_true, y_pred)\n",
    "\n",
    "print(\"Q2.2 â€” Test Metrics (original price scale)\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"MAE : {mae:.4f}\")\n",
    "\n",
    "# 4) Plot Actual vs Predicted\n",
    "\n",
    "test_start_idx = WINDOW + split_idx\n",
    "test_dates = ts.index[test_start_idx : test_start_idx + len(y_true)]\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(test_dates, y_true, label=\"Actual (Test)\", linewidth=2)\n",
    "plt.plot(test_dates, y_pred, label=\"Predicted (CNN)\", linestyle=\"--\")\n",
    "plt.title(\"Q2.2 â€” Actual vs Predicted Close (Test Set, Original Scale)\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Close Price\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Residuals over time\n",
    "errors = y_true - y_pred\n",
    "plt.figure(figsize=(12, 3))\n",
    "plt.plot(test_dates, errors)\n",
    "plt.title(\"Q2.2 â€” Prediction Errors (Actual âˆ’ Predicted)\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55549340",
   "metadata": {
    "id": "55549340"
   },
   "source": [
    "#**Q2.3** - Answer\n",
    "\n",
    "The 1D-CNN shows good capability in capturing short-term dependencies, as reflected by its relatively low MAE (18) and RMSE (22) â€” indicating that it learns local fluctuations within the 10-day input window quite effectively.\n",
    "\n",
    "However, it does not capture long-term trends as well, because a CNN processes only a fixed and local window of recent history and does not retain information beyond that window. This architectural limitation prevents it from fully modeling broader directional movements over time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf4f585",
   "metadata": {
    "id": "4cf4f585"
   },
   "source": [
    "### Question 3 - Recurrent Neural Networks (RNN) (20%)\n",
    "- Q3.1 Create an RNN model for time series forecasting. The model should include a SimpleRNN layer of size 40 and ReLU activation. Train the model using the provided dataset for 50 epochs. Use the batch_size of 16, and ADAM optimizer. Plot the training loss and validation loss. (12%)\n",
    "- Q3.2 Evaluate the model's performance using RMSE and MAE on the test set. Plot the predicted values alongside the actual values to visualize how well the model is forecasting the time series. (6%)\n",
    "- Q3.3 Why did we use a smaller batch size compared to CNN model. Why might RNN struggle with long-term dependencies, and how is this reflected in the evaluation metrics? (2%)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13dd2f5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "c13dd2f5",
    "outputId": "39879430-ea40-4df4-fde9-9cd0e7134661"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models, optimizers\n",
    "\n",
    "# 1) Prepare shapes for RNN\n",
    "#    - RNN expects (samples, timesteps, features) just like Conv1D.\n",
    "\n",
    "WINDOW = X_train.shape[1]\n",
    "\n",
    "X_train_rnn = X_train.reshape(-1, WINDOW, 1)\n",
    "X_test_rnn  = X_test.reshape(-1, WINDOW, 1)\n",
    "\n",
    "y_train_vec = y_train.ravel()\n",
    "y_test_vec  = y_test.ravel()\n",
    "\n",
    "print(\"Q3.1 â€” Shapes ready for RNN\")\n",
    "print(f\"X_train_rnn: {X_train_rnn.shape}  (samples, timesteps={WINDOW}, features=1)\")\n",
    "print(f\"X_test_rnn:  {X_test_rnn.shape}\")\n",
    "print(f\"y_train:     {y_train_vec.shape}\")\n",
    "print(f\"y_test:      {y_test_vec.shape}\")\n",
    "\n",
    "# 2) Chronological validation split from the training set (last 10% as validation)\n",
    "\n",
    "val_start = int(len(X_train_rnn) * 0.9)\n",
    "X_tr, X_val = X_train_rnn[:val_start], X_train_rnn[val_start:]\n",
    "y_tr, y_val = y_train_vec[:val_start],  y_train_vec[val_start:]\n",
    "\n",
    "print(\"\\nQ3.1 â€” Train/Val split (from training set)\")\n",
    "print(f\"X_tr:  {X_tr.shape}, y_tr:  {y_tr.shape}\")\n",
    "print(f\"X_val: {X_val.shape}, y_val:{y_val.shape}\")\n",
    "\n",
    "# 3) Define SimpleRNN model per spec\n",
    "#    - SimpleRNN(40, activation='relu')\n",
    "#    - Dense(1) for regression output\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "rnn_model = models.Sequential([\n",
    "    layers.SimpleRNN(40, activation='relu', input_shape=(WINDOW, 1)),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "rnn_model.compile(optimizer=optimizers.Adam(), loss='mse', metrics=['mae'])\n",
    "rnn_model.summary()\n",
    "\n",
    "# 4) Train for 50 epochs, batch_size=16\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "rnn_hist = rnn_model.fit(\n",
    "    X_tr, y_tr,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 5) Plot training vs validation loss (MSE)\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(rnn_hist.history['loss'], label='Train Loss (MSE)')\n",
    "plt.plot(rnn_hist.history['val_loss'], label='Validation Loss (MSE)')\n",
    "plt.title(\"Q3.1 â€” Training vs. Validation Loss (SimpleRNN)\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss (MSE)\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda71bca",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 749
    },
    "id": "cda71bca",
    "outputId": "230af335-32b7-4ed8-a755-f26d5a2b3020"
   },
   "outputs": [],
   "source": [
    "# 1) Predict on the test set (still in [0,1] scale)\n",
    "\n",
    "y_pred_scaled = rnn_model.predict(X_test_rnn, verbose=0).ravel()\n",
    "\n",
    "# 2) Inverse-transform to original price scale\n",
    "\n",
    "y_pred = y_scaler.inverse_transform(y_pred_scaled.reshape(-1, 1)).ravel()\n",
    "y_true = y_scaler.inverse_transform(y_test_vec.reshape(-1, 1)).ravel()\n",
    "\n",
    "# 3) Metrics in original units\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "mae  = mean_absolute_error(y_true, y_pred)\n",
    "\n",
    "print(\"Q3.2 â€” Test Metrics (original price scale)\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"MAE : {mae:.4f}\")\n",
    "\n",
    "# 4) Plot Actual vs Predicted\n",
    "\n",
    "test_start_idx = WINDOW + split_idx\n",
    "test_dates = ts.index[test_start_idx : test_start_idx + len(y_true)]\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(test_dates, y_true, label=\"Actual (Test)\", linewidth=2)\n",
    "plt.plot(test_dates, y_pred, label=\"Predicted (SimpleRNN)\", linestyle=\"--\")\n",
    "plt.title(\"Q3.2 â€” Actual vs Predicted Close (Test Set, Original Scale)\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Close Price\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Residuals over time\n",
    "errors = y_true - y_pred\n",
    "plt.figure(figsize=(12, 3))\n",
    "plt.plot(test_dates, errors)\n",
    "plt.title(\"Q3.2 â€” Prediction Errors (Actual âˆ’ Predicted)\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88255733",
   "metadata": {
    "id": "88255733"
   },
   "source": [
    "#**Q3.3** - Answer\n",
    "\n",
    "**1. Why smaller batch size (16 vs 32)?**\n",
    "\n",
    "RNNs process data sequentially and are more sensitive to temporal patterns than CNNs. A smaller batch size provides more frequent weight updates, helping the model better track subtle short-term sequence changes and avoid averaging away important temporal signals.\n",
    "\n",
    "**2. Why RNN struggles with long-term dependencies?**\n",
    "\n",
    "SimpleRNN has no memory gate and suffers from vanishing gradients, meaning it quickly forgets information from earlier timesteps. It mainly captures short-term behavior only. This can be seen in our metrics with the RMSE (22) and MAE (18), the model does reasonably well on short term predictions - but misses some larger or longer-term movements, confirming its limited ability to capture long-term trends.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3836425",
   "metadata": {
    "id": "c3836425"
   },
   "source": [
    "### Question 4 - Long Short-Term Memory (LSTM) (20%)\n",
    "- Q4.1 Create an LSTM model for time series forecasting.The model should include two LSTM layers of size 100 and 50, and ReLU activation. Train the model using the provided dataset for 100 epochs. Use the batch_size of 16, and ADAM optimizer. Plot the training loss and validation loss. (12%)\n",
    "- Q4.2 Evaluate the model's performance using RMSE and MAE on the test set. Plot the predicted values alongside the actual values to visualize how well the model is forecasting the time series. (6%)\n",
    "- Q4.3 Why do we use a larger epoch compared to RNN? (2%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba283c7e",
   "metadata": {
    "id": "ba283c7e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reshape inputs for LSTM\n",
    "X_train_lstm = X_train.reshape(-1, WINDOW, 1)\n",
    "X_test_lstm  = X_test.reshape(-1, WINDOW, 1)\n",
    "y_train_vec  = y_train.ravel()\n",
    "y_test_vec   = y_test.ravel()\n",
    "\n",
    "# Chronological validation split (last 10% of training)\n",
    "val_start = int(len(X_train_lstm) * 0.9)\n",
    "X_tr, X_val = X_train_lstm[:val_start], X_train_lstm[val_start:]\n",
    "y_tr, y_val = y_train_vec[:val_start],  y_train_vec[val_start:]\n",
    "\n",
    "# Build LSTM model\n",
    "lstm_model = Sequential([\n",
    "    LSTM(100, activation='relu', return_sequences=True, input_shape=(WINDOW,1)),\n",
    "    LSTM(50, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "lstm_model.compile(optimizer=Adam(), loss='mse')\n",
    "\n",
    "# Train model\n",
    "history_lstm = lstm_model.fit(\n",
    "    X_tr, y_tr,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=100,\n",
    "    batch_size=16,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Plot training vs validation loss\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(history_lstm.history['loss'], label='Train Loss')\n",
    "plt.plot(history_lstm.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.title('LSTM Training vs Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871225ef",
   "metadata": {
    "id": "871225ef"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_scaled = lstm_model.predict(X_test_lstm, verbose=0).ravel()\n",
    "y_pred = y_scaler.inverse_transform(y_pred_scaled.reshape(-1,1)).ravel()\n",
    "y_true = y_scaler.inverse_transform(y_test_vec.reshape(-1,1)).ravel()\n",
    "\n",
    "# Compute metrics\n",
    "rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "mae  = mean_absolute_error(y_true, y_pred)\n",
    "print(f\"Test RMSE: {rmse:.4f}\")\n",
    "print(f\"Test MAE : {mae:.4f}\")\n",
    "\n",
    "# Plot Actual vs Predicted\n",
    "test_start_idx = WINDOW + split_idx\n",
    "test_dates = ts.index[test_start_idx : test_start_idx + len(y_true)]\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(test_dates, y_true, label='Actual', linewidth=2)\n",
    "plt.plot(test_dates, y_pred, label='Predicted', linestyle='--')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Close Price')\n",
    "plt.title('LSTM Actual vs Predicted (Test Set)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot residuals\n",
    "errors = y_true - y_pred\n",
    "plt.figure(figsize=(10,3))\n",
    "plt.plot(test_dates, errors)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Error')\n",
    "plt.title('LSTM Prediction Errors')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f5e8c2",
   "metadata": {
    "id": "85f5e8c2"
   },
   "source": [
    "We use a larger number of epochs for LSTM models because they have more complex internal structures: memory cells and gates (input, forget, and output gates), which take longer to learn long-term dependencies in sequential data.\n",
    "LSTMs also have more parameters than traditional RNNs, so they need more training iterations (epochs) to converge and capture temporal relationships accurately without underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a6eaa4",
   "metadata": {
    "id": "a2a6eaa4"
   },
   "source": [
    "### Question 5 - Gated Recurrent Unit (GRU) (20%)\n",
    "- Q5.1 Create a GRU model for time series forecasting.The model should include two GRU layers of size 80 and 40, and ReLU activation. Train the model using the provided dataset for 100 epochs. Use the batch_size of 16, and ADAM optimizer. Plot the training loss and validation loss. (12%)\n",
    "- Q5.2 Evaluate the model's performance using RMSE and MAE on the test set. Plot the predicted values alongside the actual values to visualize how well the model is forecasting the time series.(6%)\n",
    "- Q5.3 How does the GRU compare to LSTM in terms of computational efficiency and forecasting performance? (2%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db697383",
   "metadata": {
    "id": "db697383"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63842e08",
   "metadata": {
    "id": "63842e08"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0acaf848",
   "metadata": {
    "id": "0acaf848"
   },
   "source": [
    "*Write your Answer to Q5.3 Here:*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bb2822",
   "metadata": {
    "id": "18bb2822"
   },
   "source": [
    "### Question 6 - Discussion (5%)\n",
    "- Q6.1 Compare the performance of 1D-CNN, RNN, LSTM, and GRU models based on training and test execution time, performance (RMSE and MAE), and potential overfitting/underfitting. (3%)\n",
    "- Q6.2 Discuss in which cases you would prefer using 1D-CNN vs LSTM or GRU for time series data. (2%)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe5bbd8",
   "metadata": {
    "id": "ffe5bbd8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4b39d8ed",
   "metadata": {
    "id": "4b39d8ed"
   },
   "source": [
    "*Write your Answer to Q6.2 Here:*"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
